{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NlWRHm_eDrG162vw_2z_qo8oYR9lPvT3",
      "authorship_tag": "ABX9TyMCODHxEdUT1mmxBe/i1HIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olavo-B/node2vec_MALdataset/blob/main/Node2vec__MAL_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJnXzJwSsxRd"
      },
      "source": [
        "# **Using node2vec for anime recommendations with My Anime List database**\n",
        "\n",
        "**Author**: Olavo Barros e Caio Von Rondow\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQOOj48yvKuZ"
      },
      "source": [
        "## Preparing environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYlNhSc_BmhK"
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import networkx as nx\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from community import community_louvain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqAUpJp5v-ue"
      },
      "source": [
        "## Getting and preparing data\n",
        "\n",
        "Datasets used: [Anime Recommendation Database 2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqlXzIjVwZpL"
      },
      "source": [
        "### Importing files from google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle #create the .kaggle folder in your root directory\n",
        "!echo '{\"username\":\"olavoalvesbarros\",\"key\":\"e1bf9c7933835b43262deffc6575bf85\"}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # set permissions\n",
        "!pip install kaggle #install the kaggle library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT-ZboUoQhL-",
        "outputId": "64d02d43-cffd-4e4e-c284-016a7f49de1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hernan4444/anime-recommendation-database-2020"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkApxp1TRtfX",
        "outputId": "741578ba-6819-4e2d-d491-08a7825fea99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/cli.py\", line 70, in main\n",
            "    out = args.func(**command_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1493, in dataset_download_cli\n",
            "    self.dataset_download_files(dataset,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1447, in dataset_download_files\n",
            "    self.download_file(response, outfile, quiet, not force)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1834, in download_file\n",
            "    size = int(response.headers['Content-Length'])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/_collections.py\", line 258, in __getitem__\n",
            "    val = self._container[key.lower()]\n",
            "KeyError: 'content-length'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "CvKcNJHZJ6Mf",
        "outputId": "322cfa48-c236-40a6-b182-9ee9b5027111"
      },
      "source": [
        "# This dataset only considers animes that the user has watched completely (watching_status==2) and gave it a score (score!=0)\n",
        "ZipFile('/content/anime-recommendation-database-2020.zip','r').extract('rating_complete.csv')\n",
        "\n",
        "# This dataset contain general information of every anime (17.562 different anime) like genre, stats, studio, etc\n",
        "ZipFile('/content/anime-recommendation-database-2020.zip','r').extract('anime.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c44e868877d2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This dataset only considers animes that the user has watched completely (watching_status==2) and gave it a score (score!=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/anime-recommendation-database-2020.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating_complete.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This dataset contain general information of every anime (17.562 different anime) like genre, stats, studio, etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/anime-recommendation-database-2020.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'anime.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoHlf9hgwv9d"
      },
      "source": [
        "### Creating dataframes with exported data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaJQ3vQKubJ"
      },
      "source": [
        "animes = pd.read_csv('anime.csv')\n",
        "ratings = pd.read_csv('rating_complete.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings"
      ],
      "metadata": {
        "id": "u6-VafS3JRb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qz06__8E35V"
      },
      "source": [
        "### Treating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxm5bnZwThIh"
      },
      "source": [
        "# Excluding not interesting genres in both datasets\n",
        "df = animes[animes.Genres == 'Hentai']\n",
        "ratings = ratings.drop(ratings.index[ratings.anime_id.isin(df.MAL_ID)])\n",
        "#animes = animes.drop(animes.index[animes.Genres == 'Hentai'])\n",
        "#animes = animes.drop(animes.index[animes.Score == 'Unknown'])\n",
        "\n",
        "# Saving a series of how many animes a single user rate and than geting just user\n",
        "# with the mean nuber of ratings = 185.\n",
        "# Done this due the high number of users and animes rated by them (the most active user\n",
        "# has 15k rates)\n",
        "df = ratings['user_id'].value_counts()\n",
        "df = df[df == 185]\n",
        "ratings = ratings[ratings.user_id.isin(df.keys())]\n",
        "\n",
        "# Converting animes rating to float\n",
        "#animes.Score = animes.Score.apply(lambda x:float(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZVDi_z8yxfv"
      },
      "source": [
        "print('Animes data shape: ', animes.shape)\n",
        "print('Rating data shape: ', ratings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFK185GIN4SC"
      },
      "source": [
        "# Access functions\n",
        "\n",
        "# Given an anime_id, retunrn its name\n",
        "def getAnimeName(anime_id):\n",
        "  return list(animes[animes.MAL_ID == anime_id].Name)[0]\n",
        "# Given an anime name, return its id\n",
        "def getAnimeId(anime_name):\n",
        "  return list(animes[animes.Name == anime_name].MAL_ID)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFA4cbZ63rRG"
      },
      "source": [
        "## Creating graph\n",
        "\n",
        "- Nodes: animes with rating `>= min_rating`\n",
        "- Edges: users that rating anime `x` and anime `y`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOevFfl95b7o"
      },
      "source": [
        "### Define edges weigh and nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPA4g7IS8BRQ"
      },
      "source": [
        "min_rating = 6 #int(animes.Score.mean())\n",
        "pair_frequency = defaultdict(int)\n",
        "item_frequency = defaultdict(int)\n",
        "\n",
        "# Filter instances where rating is greater than or equal to min_rating.\n",
        "rated_animes = ratings[ratings.rating >= min_rating]\n",
        "# Group instances by user.\n",
        "animes_grouped_by_users = list(rated_animes.groupby(\"user_id\"))\n",
        "for group in tqdm(\n",
        "    animes_grouped_by_users,\n",
        "    position=0,\n",
        "    leave=True,\n",
        "    desc=\"Compute anime rating frequencies\",\n",
        "):\n",
        "    # Get a list of animes rated by the user.\n",
        "    current_animes = list(group[1][\"anime_id\"])\n",
        "\n",
        "    for i in range(len(current_animes)):\n",
        "        # How many times a single anime was rated\n",
        "        item_frequency[current_animes[i]] += 1\n",
        "        for j in range(i + 1, len(current_animes)):\n",
        "            x = min(current_animes[i], current_animes[j])\n",
        "            y = max(current_animes[i], current_animes[j])\n",
        "            # How many times two animes x and y was rated by the same user z\n",
        "            pair_frequency[(x, y)] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZGKFxptyRYm"
      },
      "source": [
        "For the weights of the edges, was used [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information), which is defined as:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "        \\mathbf{pmi} (\\mathbf{x;y}) &\\equiv \\log_{2}\\left(\\frac{p(x,y)}{p(x)p(y)}\\right) \\\\\n",
        "        &\\equiv \\log_{2}p(x,y) - ( \\log_{2}p(x) + \\log_{2}p(y) ) \\\\\n",
        "        &\\equiv \\log_{2}\\left(\\frac{xy}{D}\\right) - ( \\log_{2}\\left(\\frac{x}{D}\\right) + \\log_{2}\\left(\\frac{y}{D}\\right) ) \\\\\n",
        "        &\\equiv \\log_{2}(xy) - \\log_{2}x - \\log_{2}y + \\log_{2}D\n",
        "    \\end{align}\n",
        "\n",
        "Where:\n",
        "\n",
        "  * `xy` is how many users rated both movie `x` and movie `y` with >= `min_rating`.\n",
        "  * `x` is how many users rated movie `x` >= `min_rating`.\n",
        "  * `y` is how many users rated movie `y` >= `min_rating`.\n",
        "  * `D` total number of movie ratings >= `min_rating`.\n",
        "\n",
        "\n",
        "`min_weight` is used to  reduce of node degrees, so a edge is created only if its weight `>= min_weight`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzkWw2tdUtUL"
      },
      "source": [
        "min_weight = 20\n",
        "D = math.log(sum(item_frequency.values()))\n",
        "\n",
        "# Create the movies undirected graph.\n",
        "animes_graph = nx.Graph()\n",
        "# Add weighted edges between movies.\n",
        "# This automatically adds the movie nodes to the graph.\n",
        "for pair in tqdm(\n",
        "    pair_frequency, position=0, leave=True, desc=\"Creating the movie graph\"\n",
        "):\n",
        "    x, y = pair\n",
        "    xy_frequency = pair_frequency[pair]\n",
        "    x_frequency = item_frequency[x]\n",
        "    y_frequency = item_frequency[y]\n",
        "    pmi = math.log(xy_frequency) - math.log(x_frequency) - math.log(y_frequency) + D\n",
        "    weight = pmi * xy_frequency\n",
        "    # Only include edges with weight >= min_weight.\n",
        "    if weight >= min_weight:\n",
        "        animes_graph.add_edge(x, y, weight=weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWLvWVQU34-"
      },
      "source": [
        "print(\"Total number of graph nodes:\", animes_graph.number_of_nodes())\n",
        "print(\"Total number of graph edges:\", animes_graph.number_of_edges())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVEeJ14iVBGs"
      },
      "source": [
        "degrees = []\n",
        "for node in animes_graph.nodes:\n",
        "    degrees.append(animes_graph.degree[node])\n",
        "\n",
        "print(\"Average node degree:\", round(sum(degrees) / len(degrees), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7OK4V_45t7d"
      },
      "source": [
        "### Creating a vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAbyubroVNge"
      },
      "source": [
        "vocabulary = [\"NA\"] + list(animes_graph.nodes)\n",
        "vocabulary_lookup = {token: idx for idx, token in enumerate(vocabulary)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzXMtQt9VbBD"
      },
      "source": [
        "## Random walk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Example of bias factor for return (red) and in-out (yellow) edges. (Image by linkedin.com/in/remy-liu-a24780213/)](https://miro.medium.com/max/700/1*hdhB2HLkkA9toVjRVbScBw.png)\n",
        "\n",
        "Cada caminhada pode ser entendida como uma sentença, se comparado com o Word2Vec"
      ],
      "metadata": {
        "id": "4eYlVN1EEnda"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN5dpbRIVh-S"
      },
      "source": [
        "def next_step(graph, previous, current, p, q):\n",
        "    neighbors = list(graph.neighbors(current))\n",
        "\n",
        "    weights = []\n",
        "    # Adjust the weights of the edges to the neighbors with respect to p and q.\n",
        "    for neighbor in neighbors:\n",
        "        if neighbor == previous:\n",
        "            # Control the probability to return to the previous node.\n",
        "            weights.append(graph[current][neighbor][\"weight\"] / p)\n",
        "        elif graph.has_edge(neighbor, previous):\n",
        "            # The probability of visiting a local node.\n",
        "            weights.append(graph[current][neighbor][\"weight\"])\n",
        "        else:\n",
        "            # Control the probability to move forward.\n",
        "            weights.append(graph[current][neighbor][\"weight\"] / q)\n",
        "\n",
        "    # Compute the probabilities of visiting each neighbor.\n",
        "    weight_sum = sum(weights)\n",
        "    probabilities = [weight / weight_sum for weight in weights]\n",
        "    # Probabilistically select a neighbor to visit.\n",
        "    next = np.random.choice(neighbors, size=1, p=probabilities)[0]\n",
        "    return next\n",
        "\n",
        "\n",
        "def random_walk(graph, num_walks, num_steps, p, q):\n",
        "    walks = []\n",
        "    nodes = list(graph.nodes())\n",
        "    # Perform multiple iterations of the random walk.\n",
        "    for walk_iteration in range(num_walks):\n",
        "        random.shuffle(nodes)\n",
        "\n",
        "        for node in tqdm(\n",
        "            nodes,\n",
        "            position=0,\n",
        "            leave=True,\n",
        "            desc=f\"Random walks iteration {walk_iteration + 1} of {num_walks}\",\n",
        "        ):\n",
        "            # Start the walk with a random node from the graph.\n",
        "            walk = [node]\n",
        "            # Randomly walk for num_steps.\n",
        "            while len(walk) < num_steps:\n",
        "                current = walk[-1]\n",
        "                previous = walk[-2] if len(walk) > 1 else None\n",
        "                # Compute the next node to visit.\n",
        "                next = next_step(graph, previous, current, p, q)\n",
        "                walk.append(next)\n",
        "            # Replace node ids (movie ids) in the walk with token ids.\n",
        "            walk = [vocabulary_lookup[token] for token in walk]\n",
        "            # Add the walk to the generated sequence.\n",
        "            walks.append(walk)\n",
        "\n",
        "    return walks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqSQWX09Vm17"
      },
      "source": [
        "# Random walk return parameter.\n",
        "p = 0.5\n",
        "# Random walk in-out parameter.\n",
        "# Small q value -> getting out the neighborhood\n",
        "# Large q value -> getting in the neighborhood\n",
        "q = 3\n",
        "# Number of iterations of random walks.\n",
        "num_walks = 5\n",
        "# Number of steps of each random walk.\n",
        "num_steps = 10\n",
        "walks = random_walk(animes_graph, num_walks, num_steps, p, q)\n",
        "\n",
        "print(\"Number of walks generated:\", len(walks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMywTYQPbASJ"
      },
      "source": [
        "## Skip-Gram\n",
        "\n",
        "![Resumo do Skip-Gram](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
        "\n",
        "O principal objetivo de se fazer um skip-gram é **descobrir a hidden layer (node embedding)** que possui os contextos para cada um dos nodos de dado grafo G. Assim, é possível fazer a similaridade de tais nodos, já que similares possuem contextos parecidos (com exemplo, no Word2vec, de palavras sinonimas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the model"
      ],
      "metadata": {
        "id": "j1CAwFfDFS4T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27eDbGSXasKV"
      },
      "source": [
        "def generate_examples(sequences, window_size, num_negative_samples, vocabulary_size):\n",
        "    example_weights = defaultdict(int)\n",
        "    # Iterate over all sequences (walks).\n",
        "    for sequence in tqdm(\n",
        "        sequences,\n",
        "        position=0,\n",
        "        leave=True,\n",
        "        desc=f\"Generating postive and negative examples\",\n",
        "    ):\n",
        "        # Generate positive and negative skip-gram pairs for a sequence (walk).\n",
        "        # labels -> 1 or 0, mark if a pair is a negative or positive exemple\n",
        "        # For more: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/skipgrams\n",
        "        pairs, labels = keras.preprocessing.sequence.skipgrams(\n",
        "            sequence,\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            window_size=window_size,\n",
        "            negative_samples=num_negative_samples,\n",
        "        )\n",
        "        for idx in range(len(pairs)):\n",
        "            pair = pairs[idx]\n",
        "            label = labels[idx]\n",
        "            target, context = min(pair[0], pair[1]), max(pair[0], pair[1])\n",
        "            if target == context:\n",
        "                continue\n",
        "            entry = (target, context, label)\n",
        "            example_weights[entry] += 1\n",
        "\n",
        "    targets, contexts, labels, weights = [], [], [], []\n",
        "    for entry in example_weights:\n",
        "        weight = example_weights[entry]\n",
        "        target, context, label = entry\n",
        "        targets.append(target)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "        weights.append(weight)\n",
        "\n",
        "    return np.array(targets), np.array(contexts), np.array(labels), np.array(weights)\n",
        "\n",
        "\n",
        "# num_negative_samples -> the proportion of negative samples by 1 positive sample\n",
        "num_negative_samples = 4\n",
        "targets, contexts, labels, weights = generate_examples(\n",
        "    sequences=walks,\n",
        "    window_size=num_steps,\n",
        "    num_negative_samples=num_negative_samples,\n",
        "    vocabulary_size=len(vocabulary),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJNzRWmUa3wt"
      },
      "source": [
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"Contexts shape: {contexts.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Weights shape: {weights.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkk_2y2Ma7rc"
      },
      "source": [
        "# for more https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
        "batch_size = 1024\n",
        "\n",
        "\n",
        "# Creating a dataset that fit the neural network parameters\n",
        "def create_dataset(targets, contexts, labels, weights, batch_size):\n",
        "    inputs = {\n",
        "        \"target\": targets,\n",
        "        \"context\": contexts,\n",
        "    }\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, weights))\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size * 2)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = create_dataset(\n",
        "    targets=targets,\n",
        "    contexts=contexts,\n",
        "    labels=labels,\n",
        "    weights=weights,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OaAJ1bXbKM5"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "embedding_dim = 300\n",
        "num_epochs = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eVmIu-rbVz9"
      },
      "source": [
        "def create_model(vocabulary_size, embedding_dim):\n",
        "\n",
        "    inputs = {\n",
        "        \"target\": layers.Input(name=\"target\", shape=(), dtype=\"int32\"),\n",
        "        \"context\": layers.Input(name=\"context\", shape=(), dtype=\"int32\"),\n",
        "    }\n",
        "    # Initialize item embeddings.\n",
        "    embed_item = layers.Embedding(\n",
        "        input_dim=vocabulary_size,\n",
        "        output_dim=embedding_dim,\n",
        "        embeddings_initializer=\"he_normal\",\n",
        "        embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "        name=\"item_embeddings\",\n",
        "    )\n",
        "    # Lookup embeddings for target.\n",
        "    target_embeddings = embed_item(inputs[\"target\"])\n",
        "    # Lookup embeddings for context.\n",
        "    context_embeddings = embed_item(inputs[\"context\"])\n",
        "    # Compute dot similarity between target and context embeddings.\n",
        "    logits = layers.Dot(axes=1, normalize=False, name=\"dot_similarity\")(\n",
        "        [target_embeddings, context_embeddings]\n",
        "    )\n",
        "    # Create the model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EbWDo5rbYOg"
      },
      "source": [
        "### Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W3VeZAxbarF"
      },
      "source": [
        "model = create_model(len(vocabulary), embedding_dim)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXfgcm0dbdXh"
      },
      "source": [
        "keras.utils.plot_model(\n",
        "    model, show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U22lA6Qvbgxo"
      },
      "source": [
        "history = model.fit(dataset, epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H4G8PUFhW0F"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSNH7sVDhgNe"
      },
      "source": [
        "# Getting the hidden layer of the skip-gram model\n",
        "movie_embeddings = model.get_layer(\"item_embeddings\").get_weights()[0]\n",
        "print(\"Embeddings shape:\", movie_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXfa4FCJssHP"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmL-zCQhk9t"
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open(\"embeddings.tsv\", \"w\", encoding=\"utf-8\")\n",
        "out_m = io.open(\"metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "for idx, anime_id in enumerate(vocabulary[1:]):\n",
        "    movie_title = list(animes[animes.MAL_ID == anime_id].Name)[0]\n",
        "    vector = movie_embeddings[idx]\n",
        "    out_v.write(\"\\t\".join([str(x) for x in vector]) + \"\\n\")\n",
        "    out_m.write(movie_title + \"\\n\")\n",
        "\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ACIHb6jsh3"
      },
      "source": [
        "query_movies = [\n",
        "    \"Gosick\",\n",
        "    \"K-On!\",\n",
        "    \"Kimi no Na wa.\",\n",
        "    \"Sword Art Online\",\n",
        "    \"Dragon Ball Z\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O8s2wLNkCoJ"
      },
      "source": [
        "query_embeddings = []\n",
        "\n",
        "\n",
        "for movie_title in query_movies:\n",
        "    movieId = getAnimeId(movie_title)\n",
        "    token_id = vocabulary_lookup[movieId]\n",
        "    movie_embedding = movie_embeddings[token_id]\n",
        "    query_embeddings.append(movie_embedding)\n",
        "\n",
        "query_embeddings = np.array(query_embeddings)\n",
        "len(movie_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previsão e valores do top k = 10"
      ],
      "metadata": {
        "id": "BDjyxiqcTpJc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh8huNTOkPI5"
      },
      "source": [
        "similarities = tf.linalg.matmul(\n",
        "    tf.math.l2_normalize(query_embeddings),\n",
        "    tf.math.l2_normalize(movie_embeddings),\n",
        "    transpose_b=True,\n",
        ")\n",
        "\n",
        "values, indices = tf.math.top_k(similarities, k=10)\n",
        "indices = indices.numpy().tolist()\n",
        "\n",
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gIqb-tHkRTl"
      },
      "source": [
        "for idx, title in enumerate(query_movies):\n",
        "    print(title)\n",
        "    print(\"\".rjust(len(title), \"-\"))\n",
        "    similar_tokens = indices[idx]\n",
        "    for token in similar_tokens:\n",
        "        similar_movieId = vocabulary[token]\n",
        "        similar_title = getAnimeName(similar_movieId)\n",
        "        print(f\"- {similar_title}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Source**\n",
        "\n",
        "https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/graph/ipynb/node2vec_movielens.ipynb#scrollTo=GLh4uGvnOKSN"
      ],
      "metadata": {
        "id": "RyTguFAF-3Bi"
      }
    }
  ]
}